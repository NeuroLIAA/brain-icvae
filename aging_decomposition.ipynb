{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes the script *aging_diffs.py* has already been executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from ants import image_read, image_list_to_matrix, matrix_to_images\n",
    "\n",
    "def keep_negative_values(imgs_list):\n",
    "    neg_imgs_list = []\n",
    "    for i, img in enumerate(imgs_list):\n",
    "        neg_img_mtx = img.numpy()\n",
    "        neg_img_mtx[neg_img_mtx > 0] = 0\n",
    "        neg_img_mtx = abs(neg_img_mtx)\n",
    "        neg_img = img.new_image_like(neg_img_mtx)\n",
    "        neg_imgs_list.append(neg_img)\n",
    "    return neg_imgs_list\n",
    "\n",
    "save_path = Path('aging', 'decomposition')\n",
    "mask_img = image_read('MNI152_T1_1mm_brain_mask.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_changes_dir = Path('evaluation') / 'general' / 'test' / 'age_invariant' / 'e100' / 'age_changes'\n",
    "aged_diffs = list((age_changes_dir / 'aged').glob('*.nii.gz'))\n",
    "rejuvenated_diffs = list((age_changes_dir / 'rejuvenated').glob('*.nii.gz'))\n",
    "diffs = aged_diffs + rejuvenated_diffs\n",
    "imgs_list = [image_read(str(img)) for img in diffs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imgs_matrix = image_list_to_matrix(imgs_list, mask_img)\n",
    "del imgs_list\n",
    "scaler = StandardScaler(with_std=False)\n",
    "centered_data = scaler.fit_transform(imgs_matrix)\n",
    "del imgs_matrix\n",
    "n_components = 30\n",
    "pca = PCA(n_components=n_components)\n",
    "principal_components = pca.fit_transform(centered_data)\n",
    "explained_variance = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set_style('white')\n",
    "fig, ax = plt.subplots(figsize=(6, 7))\n",
    "n_components_to_plot = n_components\n",
    "\n",
    "# Create x-axis values starting from 1\n",
    "x_positions = np.arange(1, n_components_to_plot + 1)\n",
    "variances_to_plot = explained_variance[:n_components_to_plot]\n",
    "\n",
    "# Use a more professional color scheme\n",
    "bar_color = sns.color_palette(\"deep\")[0]\n",
    "line_color = \"#143A80\"\n",
    "\n",
    "# Create bars with better spacing\n",
    "bars = ax.bar(x_positions, variances_to_plot, color=bar_color, width=1.0, alpha=0.9, \n",
    "              edgecolor='white', linewidth=0.5)\n",
    "\n",
    "# Add line plot\n",
    "ax.plot(x_positions, variances_to_plot, color=line_color, alpha=0.9, linewidth=2, \n",
    "        marker='o', markersize=3, markerfacecolor=line_color)\n",
    "\n",
    "# Improve axis styling\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(0.8)\n",
    "ax.spines['bottom'].set_linewidth(0.8)\n",
    "ax.spines['left'].set_color('#333333')\n",
    "ax.spines['bottom'].set_color('#333333')\n",
    "\n",
    "# Set axis positions\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "# Improve x-axis\n",
    "ax.set_xlim(0.5, n_components_to_plot + 0.5)\n",
    "ax.set_xticks([1, 5, 10, 15, 20, 25, 30])\n",
    "# ax.set_xlabel('Principal Component', fontsize=15, fontweight='bold', color='#333333')\n",
    "\n",
    "# Improve y-axis with percentage formatting\n",
    "max_variance = max(variances_to_plot)\n",
    "if max_variance > 0.08:\n",
    "    y_ticks = [0, 0.02, 0.04, 0.06, 0.10]\n",
    "    y_labels = ['0%', '2%', '4%', '6%', '10%']\n",
    "else:\n",
    "    y_ticks = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06]\n",
    "    y_labels = ['0%', '1%', '2%', '3%', '4%', '5%', '6%']\n",
    "\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_yticklabels(y_labels)\n",
    "ax.set_ylim(0, max_variance)\n",
    "# ax.set_ylabel('Explained Variance Ratio', fontsize=15, fontweight='bold', color='#333333')\n",
    "\n",
    "# Add grid for better readability\n",
    "# ax.grid(True, axis='y', alpha=0.3, linestyle='-', linewidth=0.5)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Improve tick appearance\n",
    "ax.tick_params(axis='both', which='major', labelsize=17, colors='#333333')\n",
    "ax.tick_params(axis='x', which='major', length=4, width=1.0)\n",
    "ax.tick_params(axis='y', which='major', length=4, width=1.0)\n",
    "\n",
    "# Add cumulative variance as text annotation\n",
    "cumulative_variance = np.cumsum(explained_variance[:30])\n",
    "ax.text(0.52, 0.98, f'First 30 PCs: {cumulative_variance[-1]:.1%}', \n",
    "        transform=ax.transAxes, fontsize=18, verticalalignment='top', \n",
    "        horizontalalignment='right', bbox=dict(boxstyle='round,pad=0.3', \n",
    "        facecolor='white', alpha=1.0, edgecolor=\"#8b8b8b\"))\n",
    "\n",
    "# Final styling\n",
    "save_path.mkdir(exist_ok=True)\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Uncomment to save the figure\n",
    "fig.savefig(save_path / 'explained_variance.png', dpi=300, bbox_inches='tight', \n",
    "            # facecolor='white', edgecolor='none', \n",
    "            transparent=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f'Explained variance ratio: {explained_variance}')\n",
    "print(f'Total explained variance: {sum(explained_variance):.3f}')\n",
    "print(f'First 5 components explain: {sum(explained_variance[:5]):.1%}')\n",
    "print(f'First 10 components explain: {sum(explained_variance[:10]):.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep negative values only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_list = keep_negative_values(imgs_list)\n",
    "neg_diffs = image_list_to_matrix(imgs_list, mask_img)\n",
    "del imgs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from numpy import save\n",
    "\n",
    "def nmf_decomposition(imgs_matrix, mask_img, n_components=3, filename='nmf_neg'):\n",
    "    nmf = NMF(n_components=n_components, init='random', random_state=0)\n",
    "    W = nmf.fit_transform(imgs_matrix)\n",
    "    H = nmf.components_\n",
    "    print(f'W shape: {W.shape}')\n",
    "    print(f'H shape: {H.shape}')\n",
    "    print(f'Number of components: {nmf.n_components_}')\n",
    "    print(f'NMF reconstruction error: {nmf.reconstruction_err_}')\n",
    "    out_path = save_path / f'{n_components}_components'\n",
    "    out_path.mkdir(exist_ok=True, parents=True)\n",
    "    print(f'Saving NMF components to {out_path}')\n",
    "    save_3d_components(H, mask_img, out_path, filename)\n",
    "    save(out_path / f'{filename}_components.npy', H)\n",
    "    save(out_path / f'{filename}_weights.npy', W)\n",
    "\n",
    "def save_3d_components(H, mask_img, out_path, filename):\n",
    "    for i in range(H.shape[0]):\n",
    "        component_3d = matrix_to_images(H[i][None, :], mask_img)[0]\n",
    "        component_3d.to_filename(str(out_path / f'{filename}{i + 1}_aging.nii.gz'))\n",
    "\n",
    "\n",
    "nmf_decomposition(neg_diffs, mask_img, n_components=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threshold components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "from numpy import stack, any, argmax, where, sum, sqrt, newaxis\n",
    "\n",
    "\n",
    "def create_thresholded_binary_maps(comp_imgs, threshold=0.001, output_file='component_{id}_binary_map.nii.gz'):\n",
    "    \"\"\"\n",
    "    Create a volume for each component where each voxel is labeled with the index of the component\n",
    "    that has the maximum value above a given threshold. 0 where all are below threshold.\n",
    "    \"\"\"\n",
    "    n_components = len(comp_imgs)\n",
    "    comp_data = [img.get_fdata() for img in comp_imgs]\n",
    "    all_comps = stack(comp_data, axis=-1)\n",
    "    above_threshold = any(all_comps > threshold, axis=-1)\n",
    "    max_comp_idx = argmax(all_comps, axis=-1) + 1  # +1 for 1-based labeling\n",
    "    thresholded_vol = where(above_threshold, max_comp_idx, 0)\n",
    "    for i in range(n_components):\n",
    "        output_file_i = output_file.format(id=i+1)\n",
    "        output_vol_i = where(thresholded_vol == (i + 1), 1, 0)\n",
    "        output_img_i = nib.Nifti1Image(output_vol_i, comp_imgs[i].affine, comp_imgs[i].header)\n",
    "        nib.save(output_img_i, output_file_i)\n",
    "\n",
    "\n",
    "def normalize_components(component_files):\n",
    "    \"\"\"\n",
    "    Normalize component data by L2 norm.\n",
    "    \"\"\"\n",
    "    comp_data = []\n",
    "    imgs = []\n",
    "    for file in component_files:\n",
    "        img = nib.load(file)\n",
    "        imgs.append(img)\n",
    "        comp_data.append(img.get_fdata())\n",
    "    all_comps = stack(comp_data, axis=-1)\n",
    "    norms = sqrt(sum(all_comps**2, axis=(0, 1, 2)))\n",
    "    normalized_comps = all_comps / norms[newaxis, newaxis, newaxis, :]\n",
    "    output_imgs = []\n",
    "    for i, img in enumerate(imgs):\n",
    "        output_img = nib.Nifti1Image(\n",
    "            normalized_comps[..., i], img.affine, img.header)\n",
    "        output_imgs.append(output_img)\n",
    "\n",
    "    return output_imgs\n",
    "\n",
    "components_path = (save_path / '3_components')\n",
    "components = list(components_path.glob('nmf_neg*_aging.nii.gz'))\n",
    "normalized_imgs = normalize_components(components)\n",
    "thresholded_output = components_path / 'component_{id}_binary_map.nii.gz'\n",
    "result = create_thresholded_binary_maps(normalized_imgs, threshold=0.001, output_file=str(thresholded_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstruct AD and HC with precomputed aging components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def load_age_changes(path):\n",
    "    age_changes_dir = Path(path)\n",
    "    diffs = list((age_changes_dir).glob('*.nii.gz'))\n",
    "    mask_img = image_read('MNI152_T1_1mm_brain_mask.nii.gz')\n",
    "    imgs_list = [image_read(str(img)) for img in diffs]\n",
    "    return imgs_list, mask_img\n",
    "\n",
    "def project_onto_components(imgs_matrix, components):\n",
    "    n_components = components.shape[0]\n",
    "    nmf_model = NMF(n_components=n_components, init='custom', random_state=0)\n",
    "    nmf_model.components_ = components\n",
    "    nmf_model.n_components_ = n_components\n",
    "    W_new = nmf_model.transform(imgs_matrix)\n",
    "    return W_new\n",
    "    \n",
    "ad_age_changes = Path('evaluation', 'diseased', 'test', 'age_invariant', 'e100', 'ad_rejuvenated')\n",
    "hc_age_changes = Path('evaluation', 'diseased', 'test', 'age_invariant', 'e100', 'hc_rejuvenated')\n",
    "aging_components = load(Path('aging', 'decomposition', '3_components', 'nmf_neg_components.npy'))\n",
    "\n",
    "ad_imgs, ad_mask = load_age_changes(ad_age_changes)\n",
    "hc_imgs, hc_mask = load_age_changes(hc_age_changes)\n",
    "ad_imgs = keep_negative_values(ad_imgs)\n",
    "hc_imgs = keep_negative_values(hc_imgs)\n",
    "ad_imgs = image_list_to_matrix(ad_imgs, ad_mask)\n",
    "hc_imgs = image_list_to_matrix(hc_imgs, hc_mask)\n",
    "ad_projected = project_onto_components(ad_imgs, aging_components)\n",
    "hc_projected = project_onto_components(hc_imgs, aging_components)\n",
    "del ad_imgs, hc_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def compare_nmf_groups(weights_group1, weights_group2, \n",
    "                       group1_name='Group 1', group2_name='Group 2',\n",
    "                       save_path=None):\n",
    "\n",
    "    # Normalize weights to percentages for easier interpretation\n",
    "    W1_norm = weights_group1 / weights_group1.sum(axis=1, keepdims=True)\n",
    "    W2_norm = weights_group2 / weights_group2.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # Create a long-form DataFrame for seaborn\n",
    "    data_list = []\n",
    "    for i in range(3):\n",
    "        for val in W1_norm[:, i]:\n",
    "            data_list.append({'Component': f'Component {i+1}', 'Normalized Weight': val, 'Group': group1_name})\n",
    "        for val in W2_norm[:, i]:\n",
    "            data_list.append({'Component': f'Component {i+1}', 'Normalized Weight': val, 'Group': group2_name})\n",
    "    \n",
    "    df = pd.DataFrame(data_list)\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(7, 6))\n",
    "    \n",
    "    # Create split violin plot with cut=0 to prevent KDE extending below zero\n",
    "    sns.violinplot(data=df, x='Component', y='Normalized Weight', hue='Group',\n",
    "                   split=True, palette=['steelblue', 'coral'],\n",
    "                   cut=0, inner='quartile', width=0.5,\n",
    "                   linecolor='black', linewidth=1.0,\n",
    "                   ax=ax)\n",
    "\n",
    "    ax.set_ylabel('Normalized Weight', fontsize=11)\n",
    "    ax.set_xlabel('')\n",
    "    ax.tick_params(axis='y', labelsize=12)\n",
    "    ax.tick_params(axis='x', labelsize=12)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.legend(loc='upper right', fontsize=12)\n",
    "\n",
    "    # Perform statistical tests and add significance markers\n",
    "    components = ['Component 1', 'Component 2', 'Component 3']\n",
    "    for i, comp in enumerate(components):\n",
    "        stat, p_value = stats.mannwhitneyu(W1_norm[:, i], W2_norm[:, i], alternative='two-sided')\n",
    "        y_max = max(W1_norm[:, i].max(), W2_norm[:, i].max())\n",
    "\n",
    "        if p_value < 0.001:\n",
    "            sig = '***'\n",
    "        elif p_value < 0.01:\n",
    "            sig = '**'\n",
    "        elif p_value < 0.05:\n",
    "            sig = '*'\n",
    "        else:\n",
    "            sig = 'ns'\n",
    "\n",
    "        ax.text(i, y_max * 1.02, sig, ha='center', fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path / 'nmf_violinplot_comparison.png', dpi=300, bbox_inches='tight', transparent=True)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STATISTICAL COMPARISON SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    for i in range(3):\n",
    "        print(f\"\\nComponent {i+1}:\")\n",
    "        print(f\"  {group1_name}: mean={W1_norm[:, i].mean():.3f}, std={W1_norm[:, i].std():.3f}\")\n",
    "        print(f\"  {group2_name}: mean={W2_norm[:, i].mean():.3f}, std={W2_norm[:, i].std():.3f}\")\n",
    "\n",
    "        stat, p_value = stats.mannwhitneyu(W1_norm[:, i], W2_norm[:, i], alternative='two-sided')\n",
    "        cohens_d = (W1_norm[:, i].mean() - W2_norm[:, i].mean()) / \\\n",
    "                   np.sqrt((W1_norm[:, i].std()**2 + W2_norm[:, i].std()**2) / 2)\n",
    "        print(f\"  Mann-Whitney U test: U={stat:.2f}, p={p_value:.4f}\")\n",
    "        print(f\"  Effect size (Cohen's d): {cohens_d:.3f}\")\n",
    "\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "compare_nmf_groups(ad_projected, hc_projected,\n",
    "                   group1_name='Alzheimer\\'s Disease', \n",
    "                   group2_name='Healthy Controls',\n",
    "                   save_path=save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
